\section{Microservices are not the silver bullet}
Over the last decade Microservices architecture has gain great popularity to the extent where very little people question if this architecture is the right fit for the particular project and if there might be something better suited for the task. One reason for this might be the incredible flood of articles on the internet with primer focus on positive aspects of Microservices and how they can solve (almost) every problem, while not talking much about possible negative aspects. The important note here is that most of the articles and successful stories are in fact coming from big corporation with enormous amount of resources (e.g. Netflix, Amazon, Coca-Cola), which none of the small/medium companies can match, while also those smaller companies are facing absolutely different challenges then the inter-national corporations.

Microservices are very tightly connected to modern concept of Cloud computing, which has become multi-billion market (\$545 billion in 2022 \cite{CC_MARKET_SIZE}) with potential to bring huge savings, but also huge expenses when not used properly. And let's not pretend, those Cloud service providers are off course trying to convince us to convert into our infrastructure  and systems into new modern technologies in-order to increase their profits. This is nothing new, since everyone is just trying to make money, but we should be aware of their intentions when they are trying to convince us to use their technologies - which in case of Microservice and their complexity of operation and deployment, we will be inevitably forced to use their services, but with Monoliths we are usually capable to run everything ourselves.

In order to present some concrete data let's look into a couple of real-world examples where Microservices were not deemed as the right solution.

\begin{example}[Amazon Prime]
    An example where Microservices have proven to be too costly choice and whole application had to be migrated to Monolithic architecture in order for the system to be efficient is from Amazon. On March 22 (2023) Amazons video stream service called Prime Video, published article on their technology blog with headline `Scaling up the Prime Video audio/video monitoring service and reducing costs by 90\%'. At Prime Video, they offer thousands of live streams to their customers. To ensure that customers seamlessly receive content, Prime Video set up a tool to monitor every stream viewed by customers to identify perceptual quality issues. \cite{AMAZON_ARTICLE}

    The initial version of the service consisted of distributed components that were orchestrated by \textit{AWS Step Functions}. In there, this would allow to scale each component independently. However, the way they used components caused them to hit a hard scaling limit at around 5\,\% of the expected load. Also, the overall cost of all the building blocks was too high to use it at a large scale. The two most expensive operations in terms of cost were the orchestration workflow and data passing between distributed components. To address this issue, they moved all components into a single process to keep the data transfer within the process memory, which also simplified the orchestration logic. Since now all operations were compiled into a single process, they could rely on scalable Amazon compute (EC2) and container (ECS) instances for the deployment. \cite{AMAZON_ARTICLE}

    Conceptually, the high-level architecture remained the same. They still have exactly the same components as in the initial design (media conversion, detectors, orchestration). This allowed them to reuse a lot of code and quickly migrate. Originally, they could scale several detectors horizontally, as each of them ran as a separate microservice. However, in new approach the number of detectors only scale vertically because all run within the same instance, and it would quickly exceed the capacity of a single instance. This limitation was overcome by running multiple instances and by implementing lightweight orchestration layer to distribute customer requests. Overall the migration from distributed Microservice architecture to Monolith let them save cost of infrastructure over 90\,\% and increased scaling capabilities. \cite{AMAZON_ARTICLE}
\end{example}

\begin{example}[Shopify]
    An example of company who still run successfully Monolithic system with thousands of developers is Shopify. It is a complete commerce platform, that enables businesses to build an online store, market to customers and accept payments. The article\cite{SHOPIFY_MONOLITH_ARTICLE} with the most insights of their architecture comes from Sep 16 2020, and they are writing regularly more insight stories about their Monolith on their \textit{Shopify Engineering}\cite{SHOPIFY_ENGINEERING} website.

    They have a massive Monolith written in Ruby on Rails framework, just its core is having over 2.8 million lines of Ruby code. This is one of the oldest, largest Rails codebases on the planet, under continuous development since at least 2006. Rails doesn't provide patterns or tooling for managing the inherent complexity and adding features in a structured, well-bounded way. That's why in 2017, Shopify founded a team to investigate how to make their Rails monoliths more modular. The goal was to help scale towards ever-increasing system capabilities and complexity by creating smaller, independent units of code they called components. The added constraints on how they write code triggered deep software design discussions throughout the organization. This resulted in mindset shift across developers towards stronger focus on modular design. Clearly defined ownership for areas of the codebase was one of the key factor for successful transition. \cite{SHOPIFY_MONOLITH_ARTICLE}

    They started out by focusing on building a clean public interface around each component to hide the internals. The expectation was that changing internals of a component wouldn't break other components, and it would be easier to understand the behavior of a component in isolation. They had to balance the encapsulation with dependency graph to avoid circular dependencies, which are very risky since change to any component within chain can break all other components. Different techniques like inversion of control and publish/subscribe mechanism were introduced to helped minimize relations and decrease coupling. At the end they ended up with 37 components in the main monolith, and they are very deliberate about splitting functionality out into separate services due to overall complexity of distributed system of services. \cite{SHOPIFY_MONOLITH_ARTICLE}
\end{example}

\begin{note*}
    Netflix is one of the most mentioned company for its Microservices architecture. What is not that well know, is that the actual migration from Monolithic system, when they started to having trouble with performance and scaling was in 2009. By that time they had Monolith for 10 years and in that time already over 11~millions \cite{NETFLIX_2009_EARNINGS} of paying subscribers. No one knows what would happen if they started the original architecture with something else, maybe it would work, but definitely the Monolith worked well until it just didn't suit their need anymore.
\end{note*}
